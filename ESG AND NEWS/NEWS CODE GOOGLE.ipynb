{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Amazon_news**"
      ],
      "metadata": {
        "id": "FEtFKcxcXWFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9lwLH7uM9La",
        "outputId": "3795a084-9caf-46e1-e96c-0f4e9e78845c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GDELT daily trading-day download (1 news per day) for AMAZON...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "days (calendar): 100%|██████████| 2192/2192 [37:15<00:00,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished.\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved CSV: amazon_news_2019_2024_1news_per_tradingday.csv Size (MB): 0.41716480255126953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# gdelt_amazon_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- USER CONFIG ----------\n",
        "QUERY = 'AMAZON'                    # company keyword\n",
        "START_DATE = datetime(2019, 1, 1)   # from 2019\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"amazon_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10            # we will later keep only 1 row/day\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------------\n",
        "\n",
        "# requests session with retries\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-amazon-fetch-v4/1.0 (+https://example)\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    \"\"\"\n",
        "    Build GDELT URL for a specific date window.\n",
        "    Here we enforce English only via sourcelang:english\n",
        "    \"\"\"\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    url = (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "    return url\n",
        "\n",
        "def fetch_csv_text(url, timeout=30):\n",
        "    \"\"\"Download a CSV string from GDELT\"\"\"\n",
        "    try:\n",
        "        r = session.get(url, timeout=timeout)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        text = r.text\n",
        "        if not text.strip():\n",
        "            return None\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv_to_df(text):\n",
        "    \"\"\"Convert GDELT CSV response to pandas DataFrame\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_one_article_for_day(day_dt):\n",
        "    \"\"\"\n",
        "    Fetch up to MAXRECORDS_PER_DAY articles for a single day,\n",
        "    filter to English, and return AT MOST ONE row (or None if no news).\n",
        "    \"\"\"\n",
        "    # full-day window [00:00:00, 23:59:59] for that calendar day\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv_to_df(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only (extra safety, though query already has sourcelang:english)\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # ---- choose exactly one article for that day ----\n",
        "    # If there is a date column, we can sort by it; otherwise just take the first row.\n",
        "    # Common candidates: 'SQLDATE', 'DATE', or 'DocumentIdentifier' ordering.\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        # SQLDATE is usually yyyymmdd\n",
        "        df = df.sort_values('SQLDATE', ascending=True)\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date', ascending=True)\n",
        "\n",
        "    one_row = df.head(1).copy()\n",
        "\n",
        "    # Add the query date explicitly so it's easy to join with price data later\n",
        "    one_row['QueryDate'] = day_dt.date()\n",
        "\n",
        "    return one_row\n",
        "\n",
        "def run_daily_tradingday_download():\n",
        "    \"\"\"\n",
        "    Loop over every day from START_DATE to END_DATE.\n",
        "    - Only Monday–Friday (approx trading days)\n",
        "    - For each such day, keep at most 1 article\n",
        "    - Save all days into a single CSV at the end\n",
        "    \"\"\"\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    # approximate number of days in range just for progress bar\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"days (calendar)\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # skip weekends (Sat=5, Sun=6)\n",
        "        if current.weekday() < 5:\n",
        "            # approx trading day -> try to get exactly 1 news row\n",
        "            try:\n",
        "                row_df = get_one_article_for_day(current)\n",
        "            except Exception as e:\n",
        "                print(\"Error for day\", current.date(), \":\", e)\n",
        "                row_df = None\n",
        "\n",
        "            if row_df is not None and not row_df.empty:\n",
        "                all_rows.append(row_df)\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No articles found in the entire range.\")\n",
        "        return\n",
        "\n",
        "    final_df = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    # Just in case, enforce max 1 row per QueryDate by grouping and taking first\n",
        "    final_df = (\n",
        "        final_df.sort_values('QueryDate')\n",
        "                .groupby('QueryDate', as_index=False)\n",
        "                .first()\n",
        "    )\n",
        "\n",
        "    final_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Finished.\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final_df))\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        print(\"Saved CSV:\", OUT_CSV, \"Size (MB):\", os.path.getsize(OUT_CSV) / (1024*1024))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GDELT daily trading-day download (1 news per day) for AMAZON...\")\n",
        "    run_daily_tradingday_download()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"amazon_news_2019_2024_1news_per_tradingday.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1lA_VbkeNAR-",
        "outputId": "fe1adb6a-0911-4592-eac0-874fca286126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f793ce5-c386-48be-b86b-a4d5fb25ee82\", \"amazon_news_2019_2024_1news_per_tradingday.csv\", 437429)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kckRAY9YV7MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apple_News**"
      ],
      "metadata": {
        "id": "Np23wLT3XfqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_apple_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- USER CONFIG ----------\n",
        "QUERY = 'APPLE'                    # company keyword\n",
        "START_DATE = datetime(2019, 1, 1)  # from 2019\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"apple_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10           # we later keep only 1 row/day\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------------\n",
        "\n",
        "# requests session with retries\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-apple-fetch-v1/1.0 (+https://example)\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    \"\"\"\n",
        "    Build GDELT URL for a specific date window.\n",
        "    Enforces English via sourcelang:english\n",
        "    \"\"\"\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    url = (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "    return url\n",
        "\n",
        "def fetch_csv_text(url, timeout=30):\n",
        "    \"\"\"Download a CSV string from GDELT\"\"\"\n",
        "    try:\n",
        "        r = session.get(url, timeout=timeout)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        text = r.text\n",
        "        if not text.strip():\n",
        "            return None\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv_to_df(text):\n",
        "    \"\"\"Convert GDELT CSV response to pandas DataFrame\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "        return df\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_one_article_for_day(day_dt):\n",
        "    \"\"\"\n",
        "    Fetch up to MAXRECORDS_PER_DAY articles for a single day,\n",
        "    filter to English, and return AT MOST ONE row.\n",
        "    \"\"\"\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv_to_df(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by date if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE', ascending=True)\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date', ascending=True)\n",
        "\n",
        "    # pick first row\n",
        "    one_row = df.head(1).copy()\n",
        "\n",
        "    # Add QueryDate for merging with price data later\n",
        "    one_row['QueryDate'] = day_dt.date()\n",
        "\n",
        "    return one_row\n",
        "\n",
        "def run_daily_tradingday_download():\n",
        "    \"\"\"\n",
        "    Loop through every trading day (Mon–Fri) between START_DATE and END_DATE.\n",
        "    Save at most 1 news article per day.\n",
        "    \"\"\"\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"days (calendar)\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "\n",
        "        # trading day = Monday–Friday\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                row_df = get_one_article_for_day(current)\n",
        "            except Exception as e:\n",
        "                print(\"Error for day\", current.date(), \":\", e)\n",
        "                row_df = None\n",
        "\n",
        "            if row_df is not None and not row_df.empty:\n",
        "                all_rows.append(row_df)\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No articles found in the entire range.\")\n",
        "        return\n",
        "\n",
        "    final_df = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    # ensure 1 row per QueryDate\n",
        "    final_df = (\n",
        "        final_df.sort_values('QueryDate')\n",
        "                .groupby('QueryDate', as_index=False)\n",
        "                .first()\n",
        "    )\n",
        "\n",
        "    final_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Finished.\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final_df))\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        print(\"Saved CSV:\", OUT_CSV, \"Size (MB):\", os.path.getsize(OUT_CSV) / (1024*1024))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GDELT daily trading-day download (1 news per day) for APPLE...\")\n",
        "    run_daily_tradingday_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzls2WPaXjBF",
        "outputId": "87c43853-0260-4d08-fec4-f06954b6266c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GDELT daily trading-day download (1 news per day) for APPLE...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "days (calendar): 100%|██████████| 2192/2192 [36:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished.\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved CSV: apple_news_2019_2024_1news_per_tradingday.csv Size (MB): 0.40357017517089844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"apple_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "z1ItNNy6gPur",
        "outputId": "60526048-a0a4-462c-febb-3a310020968e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bbc0c196-5cff-4a33-9022-d7e4cae507cc\", \"apple_news_2019_2024_1news_per_tradingday.csv\", 423174)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CiAZk2_gQT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google**"
      ],
      "metadata": {
        "id": "WEQdlzYpgf30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_google_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'GOOGLE OR ALPHABET'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"google_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-google-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        if current.weekday() < 5:  # Monday–Friday\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found!\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False)\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with news:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GOOGLE daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIU75pZjghV3",
        "outputId": "0b2ba9aa-3d5e-4577-9425-013ead9c153b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GOOGLE daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [20:40<00:00,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No news found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_google_1news_per_tradingday_2019_2024_fixed.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'GOOGLE'   # simpler, like your AMAZON query\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"google_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-google-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # same trick as before: enforce English via sourcelang:english\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or try a closer date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GOOGLE daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8BRPBhgh_g",
        "outputId": "4e7ca5bf-4aa6-48c4-e46c-1fa450875f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GOOGLE daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [38:45<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: google_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"google_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RelhnYoLl9Dp",
        "outputId": "810e1e03-c394-4eaa-99e6-aa885efcebdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a01d5d95-fc4e-47ba-86a6-0aef8f0104bc\", \"google_news_2019_2024_1news_per_tradingday.csv\", 427935)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8o8_WiyNwBqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nvidia**"
      ],
      "metadata": {
        "id": "9qEUWgO4wfCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_nvidia_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'NVIDIA'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"nvidia_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-nvidia-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting NVIDIA daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLYNUNSIwg90",
        "outputId": "13991faf-403a-456f-d439-d0415f722384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NVIDIA daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [33:38<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: nvidia_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"nvidia_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AOh_j1ccwhur",
        "outputId": "3b760952-92c8-42ee-ca3b-dc6f57b62148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7e52b1a9-0813-4598-a966-e06f264f6049\", \"nvidia_news_2019_2024_1news_per_tradingday.csv\", 394604)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_microsoft_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'MICROSOFT'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"microsoft_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-microsoft-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting MICROSOFT daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FUW6CSv4aA8",
        "outputId": "3d9c4c9c-ed25-49aa-aa1b-75f1628ddc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting MICROSOFT daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [35:03<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: microsoft_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"microsoft_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VhV9UAPV5TYQ",
        "outputId": "704ed1a6-13e4-432b-e0ae-c76866606be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75723ebb-b9c1-4d98-94ce-9e86ffe4bff5\", \"microsoft_news_2019_2024_1news_per_tradingday.csv\", 429318)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0tqJMZ7WySE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**S&P 500**"
      ],
      "metadata": {
        "id": "P7xx8X73Xmj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_sp500_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# Phrase search for the index itself\n",
        "QUERY = '\"S&P 500\"'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"sp500_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-sp500-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – try adjusting QUERY or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting S&P 500 daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOXZAUNiXnyZ",
        "outputId": "bc71ace0-d0d8-47cd-9a68-5d2e01857c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting S&P 500 daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [36:47<00:00,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: sp500_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sp500_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GybdpvMpXyLj",
        "outputId": "b5af3ff1-862c-4e51-9715-4aff918c5371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7c5be374-bebb-408d-a6df-b21773fe8631\", \"sp500_news_2019_2024_1news_per_tradingday.csv\", 427301)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXK9dixuhAnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AstraZeneca PLO**"
      ],
      "metadata": {
        "id": "JgNRlfuXhU75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_astrazeneca_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'ASTRAZENECA'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"astrazeneca_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-astrazeneca-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting ASTRAZENECA daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6BNt5BohY2W",
        "outputId": "97204482-7a90-421b-9640-dc6d92953e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ASTRAZENECA daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [36:33<00:00,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: astrazeneca_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"astrazeneca_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4nyIs6sThiaQ",
        "outputId": "e471550c-4e3c-49b8-8ccc-e9631156a2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_579e031d-38d6-4ccf-a672-c7d25ddef2e3\", \"astrazeneca_news_2019_2024_1news_per_tradingday.csv\", 426984)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AuSWKFGLqXNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HSBC**"
      ],
      "metadata": {
        "id": "2lyYbnbIiLex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_hsbc_holdings_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# Slightly focused query for the group\n",
        "QUERY = '\"HSBC HOLDINGS\"'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"hsbc_holdings_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-hsbc-holdings-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – try adjusting QUERY or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting HSBC HOLDINGS daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMwuqrtRiPUv",
        "outputId": "0aaa88c1-007b-426f-834d-f995a15345e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting HSBC HOLDINGS daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [42:57<00:00,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1552\n",
            "Saved to: hsbc_holdings_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"hsbc_holdings_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zVpKUhfLiQGw",
        "outputId": "4ac6d26c-a5f6-4cf9-9338-66945248fc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c5552c9-7747-48b7-82ee-8232fec42b05\", \"hsbc_holdings_news_2019_2024_1news_per_tradingday.csv\", 398722)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Htp6NoOwsobT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linde PLC**"
      ],
      "metadata": {
        "id": "5zf6G2ZOtGaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_linde_plc_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"LINDE PLC\"'   # you can change to 'LINDE' if this is too narrow\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"linde_plc_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-linde-plc-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – you might try QUERY = 'LINDE' instead.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting LINDE PLC daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc32SRfUtIpu",
        "outputId": "2119d74d-7474-4331-dfe5-6cea0e652c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LINDE PLC daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [52:23<00:00,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1285\n",
            "Saved to: linde_plc_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"linde_plc_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Etw8CrsJtPCo",
        "outputId": "58fe6fa8-a26a-4ee7-e7d0-55c35f6a2b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_01d33472-361c-4593-bbea-acdc048bdeda\", \"linde_plc_news_2019_2024_1news_per_tradingday.csv\", 347204)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shell PLC**"
      ],
      "metadata": {
        "id": "I8-6JtHI5ng1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_shell_plc_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"SHELL PLC\"'   # you can change to 'SHELL' or '\"ROYAL DUTCH SHELL\"' if needed\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"shell_plc_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-shell-plc-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – you might try QUERY = 'SHELL' or '\\\"ROYAL DUTCH SHELL\\\"' instead.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting SHELL PLC daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkS_uUIp5PDV",
        "outputId": "ba991e22-4be6-41fb-c8e3-d008d6c532f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting SHELL PLC daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days:  86%|████████▋ | 1892/2192 [36:02<10:10,  2.04s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22SHELL+PLC%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20240307000000&enddatetime=20240307235959\n",
            "Processing days:  86%|████████▋ | 1893/2192 [37:00<1:17:12, 15.49s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22SHELL+PLC%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20240308000000&enddatetime=20240308235959\n",
            "Processing days:  86%|████████▋ | 1894/2192 [37:15<1:15:41, 15.24s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22SHELL+PLC%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20240311000000&enddatetime=20240311235959\n",
            "Processing days:  87%|████████▋ | 1906/2192 [40:10<1:10:31, 14.80s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22SHELL+PLC%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20240321000000&enddatetime=20240321235959\n",
            "Processing days:  90%|████████▉ | 1967/2192 [44:20<09:21,  2.49s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22SHELL+PLC%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20240521000000&enddatetime=20240521235959\n",
            "Processing days: 100%|██████████| 2192/2192 [50:15<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1549\n",
            "Saved to: shell_plc_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"shell_plc_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sBpsOyuO5jGE",
        "outputId": "ce650bee-96d5-42b0-8845-72a9b0e277bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_630c96b9-b733-4b1a-afed-d3412227e6a6\", \"shell_plc_news_2019_2024_1news_per_tradingday.csv\", 404888)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unilever PLO**"
      ],
      "metadata": {
        "id": "cz8oY_3WFVxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_unilever_plc_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"UNILEVER PLC\"'   # if too narrow, change to 'UNILEVER'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"unilever_plc_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-unilever-plc-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – you might try QUERY = 'UNILEVER' instead.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting UNILEVER PLC daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3qV-HGZFNt2",
        "outputId": "9bfc4b89-ce5c-468a-da94-2985ce353df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting UNILEVER PLC daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [39:51<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1450\n",
            "Saved to: unilever_plc_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"unilever_plc_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fsx1cTW7FYMu",
        "outputId": "facd867f-cd49-4e3a-9578-6cee93d5f6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff864a01-6917-4b04-b3ff-ba5887f8c3b2\", \"unilever_plc_news_2019_2024_1news_per_tradingday.csv\", 407033)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "companies = [\n",
        "    (\"AMAZON\", \"amazon_news_...csv\"),\n",
        "    (\"APPLE\", \"apple_news_...csv\"),\n",
        "    ...\n",
        "    ('\"UNILEVER PLC\"', \"unilever_plc_news_...csv\"),\n",
        "]\n"
      ],
      "metadata": {
        "id": "Pjx5qrS1bydZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "37eab4df-9a57-41c3-ea6e-ba37282e6022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:4: SyntaxWarning: 'ellipsis' object is not callable; perhaps you missed a comma?\n",
            "<>:4: SyntaxWarning: 'ellipsis' object is not callable; perhaps you missed a comma?\n",
            "/tmp/ipython-input-3594943482.py:4: SyntaxWarning: 'ellipsis' object is not callable; perhaps you missed a comma?\n",
            "  ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'ellipsis' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3594943482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"AMAZON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"amazon_news_...csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"APPLE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apple_news_...csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'\"UNILEVER PLC\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unilever_plc_news_...csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0CRLTvAWIwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FTSE100**"
      ],
      "metadata": {
        "id": "xGYY4EDSWj-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_ftse_100_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"FTSE 100\"'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"ftse_100_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-ftse-100-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting FTSE 100 daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKWayT8WmhT",
        "outputId": "99c655ff-9b1d-49cb-d78b-34c6e650254d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting FTSE 100 daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [51:57<00:00,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: ftse_100_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"ftse_100_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3mYctVmVWtLe",
        "outputId": "db652994-f12f-4ca7-b7ed-30ff5f7d4e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ebaa7e66-d77e-41d0-8b17-4ad0de3b361a\", \"ftse_100_news_2019_2024_1news_per_tradingday.csv\", 388145)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_toyota_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = 'TOYOTA MOTOR CORPORATION'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"toyota_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-toyota-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check the QUERY or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting TOYOTA MOTOR CORPORATION daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrrPORpB5bRj",
        "outputId": "50ed5ed1-f7c2-4cb5-93ca-45657ddccaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting TOYOTA MOTOR CORPORATION daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [41:43<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1542\n",
            "Saved to: toyota_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"toyota_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PNY13nNI5rCi",
        "outputId": "6213aa05-0e4d-46bf-95ec-60ced77a264b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea37c726-126e-437f-9739-2a99ae44a1ed\", \"toyota_news_2019_2024_1news_per_tradingday.csv\", 379678)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_TgLCBDEmLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mitsubishi**\n"
      ],
      "metadata": {
        "id": "MyixNPF1Es6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_mufg_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"Mitsubishi UFJ Financial Group\"'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"mufg_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-mufg-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # enforce English sources\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Mitsubishi UFJ Financial Group (MUFG) daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1LjMBdXEvRu",
        "outputId": "1f3372fc-bb23-4398-b0d3-e987d5949489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Mitsubishi UFJ Financial Group (MUFG) daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days:  79%|███████▉  | 1737/2192 [28:21<08:24,  1.11s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Mitsubishi+UFJ+Financial+Group%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20231004000000&enddatetime=20231004235959\n",
            "Processing days:  79%|███████▉  | 1739/2192 [29:38<1:52:11, 14.86s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Mitsubishi+UFJ+Financial+Group%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20231006000000&enddatetime=20231006235959\n",
            "Processing days:  80%|███████▉  | 1743/2192 [30:45<1:49:57, 14.69s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Mitsubishi+UFJ+Financial+Group%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20231010000000&enddatetime=20231010235959\n",
            "Processing days: 100%|██████████| 2192/2192 [42:22<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1330\n",
            "Saved to: mufg_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRWvVUPHR1Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sony Group**"
      ],
      "metadata": {
        "id": "819IvYDxSRXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_sony_group_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"SONY GROUP CORPORATION\"'   # you can change to 'SONY' if too narrow\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"sony_group_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-sony-group-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.text if r.text.strip() else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – check QUERY or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf‑8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting SONY GROUP CORPORATION daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewXUggOZSMfi",
        "outputId": "8b776843-1131-4c9a-d400-ecca315cee5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SONY GROUP CORPORATION daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [34:41<00:00,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 603\n",
            "Saved to: sony_group_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sony_group_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "j5kDistBSNnJ",
        "outputId": "51a34c76-c577-438c-9f3d-8060f026e6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8d1dbfc5-e0e5-4d5e-8b54-cde6dd5d33cb\", \"sony_group_news_2019_2024_1news_per_tradingday.csv\", 160838)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HITACHI**"
      ],
      "metadata": {
        "id": "WUTm0QWedW7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_hitachi_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"HITACHI LTD.\"'   # you can also try 'HITACHI' if needed\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV = \"hitachi_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-hitachi-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        text = r.text\n",
        "        if not text.strip():\n",
        "            return None\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found – you may need to adjust QUERY\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf‑8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting HITACHI LTD daily news download…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1md7NzKczRc",
        "outputId": "6c1a5e80-11a9-4a1a-945d-a09013db4fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting HITACHI LTD daily news download…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days:   9%|▉         | 199/2192 [03:18<38:52,  1.17s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22HITACHI+LTD.%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20190719000000&enddatetime=20190719235959\n",
            "Processing days:  46%|████▌     | 1003/2192 [21:04<30:54,  1.56s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22HITACHI+LTD.%22+sourcelang%3Aenglish&mode=artlist&maxrecords=10&format=csv&startdatetime=20210930000000&enddatetime=20210930235959\n",
            "Processing days: 100%|██████████| 2192/2192 [45:03<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1512\n",
            "Saved to: hitachi_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"hitachi_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pShNAFlUdT-s",
        "outputId": "46c63cac-77a1-4135-c8ce-2db04f24f1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5154ed13-2c23-4100-ac2d-b3d992552bf9\", \"hitachi_news_2019_2024_1news_per_tradingday.csv\", 442478)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PCdic5KoYt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nikkei**"
      ],
      "metadata": {
        "id": "qiiinzOYo3if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_nikkei_1news_per_tradingday_2019_2024_FIXED.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# Broader query than just \"Nikkei 225\"\n",
        "QUERY = 'NIKKEI'\n",
        "START_DATE = datetime(2019, 1, 1)\n",
        "END_DATE   = datetime(2024, 12, 31)\n",
        "OUT_CSV    = \"nikkei_news_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP      = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-nikkei-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # still constrain to English sources via sourcelang:english\n",
        "    q = quote_plus(f'{query} sourcelang:english')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        text = r.text\n",
        "        if not text.strip():\n",
        "            return None\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # keep English-only if column exists\n",
        "    if 'DocumentLanguage' in df.columns:\n",
        "        df = df[df['DocumentLanguage'].astype(str).str.lower() == 'english']\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    # pick just 1 article\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        # Monday–Friday only (approx trading days)\n",
        "        if current.weekday() < 5:\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error for day {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found at all – even with broader query. Then GDELT coverage for this term may be very sparse.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Nikkei daily news download (broader query: NIKKEI)…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLCY2k7yyiFa",
        "outputId": "cb92e27c-4031-4424-a2e1-2a82067d01c3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Nikkei daily news download (broader query: NIKKEI)…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days: 100%|██████████| 2192/2192 [47:40<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1564\n",
            "Saved to: nikkei_news_2019_2024_1news_per_tradingday.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"nikkei_news_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tGrTm5DCy8Os",
        "outputId": "f5a452cd-9381-4ec4-c51e-8eecb4d9ce3d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d4b5e40-2699-43e1-bca8-9faae8c2d152\", \"nikkei_news_2019_2024_1news_per_tradingday.csv\", 421727)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nintendo**"
      ],
      "metadata": {
        "id": "x6psQSAIGV3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gdelt_nintendo_no_lang_filter_1news_per_tradingday_2019_2024.py\n",
        "import os\n",
        "import time\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "QUERY = '\"Nintendo\"'  # Broaden query\n",
        "START_DATE = datetime(2019, 1, 1)  # Start with Jan 2019\n",
        "END_DATE   = datetime(2024, 12, 31)  # End with Dec 2024\n",
        "OUT_CSV = \"nintendo_news_all_lang_2019_2024_1news_per_tradingday.csv\"\n",
        "MAXRECORDS_PER_DAY = 10\n",
        "REQUEST_SLEEP = 0.6\n",
        "# ----------------------------\n",
        "\n",
        "session = requests.Session()\n",
        "retries = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=1.0,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        "    allowed_methods=[\"GET\"]\n",
        ")\n",
        "session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
        "session.headers.update({\"User-Agent\": \"gdelt-nintendo-fetch/1.0\"})\n",
        "\n",
        "def make_url(query, start_dt, end_dt, maxrecords=MAXRECORDS_PER_DAY, fmt=\"csv\"):\n",
        "    # No language filter, so we fetch all languages\n",
        "    q = quote_plus(f'{query}')\n",
        "    start_s = start_dt.strftime(\"%Y%m%d\") + \"000000\"\n",
        "    end_s   = end_dt.strftime(\"%Y%m%d\") + \"235959\"\n",
        "    return (\n",
        "        f\"http://api.gdeltproject.org/api/v2/doc/doc\"\n",
        "        f\"?query={q}\"\n",
        "        f\"&mode=artlist\"\n",
        "        f\"&maxrecords={maxrecords}\"\n",
        "        f\"&format={fmt}\"\n",
        "        f\"&startdatetime={start_s}\"\n",
        "        f\"&enddatetime={end_s}\"\n",
        "    )\n",
        "\n",
        "def fetch_csv_text(url):\n",
        "    try:\n",
        "        r = session.get(url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        text = r.text\n",
        "        if not text.strip():\n",
        "            return None\n",
        "        return text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_csv(text):\n",
        "    try:\n",
        "        return pd.read_csv(StringIO(text), on_bad_lines='skip')\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def get_one_news(day_dt):\n",
        "    url = make_url(QUERY, day_dt, day_dt)\n",
        "    text = fetch_csv_text(url)\n",
        "    time.sleep(REQUEST_SLEEP)\n",
        "\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    df = parse_csv(text)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "\n",
        "    # Sort by some date-like column if available\n",
        "    if 'SQLDATE' in df.columns:\n",
        "        df = df.sort_values('SQLDATE')\n",
        "    elif 'Date' in df.columns:\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "    row = df.head(1).copy()\n",
        "    row['QueryDate'] = day_dt.date()\n",
        "    return row\n",
        "\n",
        "def run_download():\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        os.remove(OUT_CSV)\n",
        "\n",
        "    all_rows = []\n",
        "    current = START_DATE\n",
        "\n",
        "    total_days = (END_DATE - START_DATE).days + 1\n",
        "    pbar = tqdm(total=total_days, desc=\"Processing days\")\n",
        "\n",
        "    while current <= END_DATE:\n",
        "        if current.weekday() < 5:  # Monday–Friday\n",
        "            try:\n",
        "                r = get_one_news(current)\n",
        "                if r is not None and not r.empty:\n",
        "                    all_rows.append(r)\n",
        "            except Exception as e:\n",
        "                print(f\"Error for day {current.date()}: {e}\")\n",
        "\n",
        "        current += timedelta(days=1)\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"No news found — please check query or date range.\")\n",
        "        return\n",
        "\n",
        "    final = pd.concat(all_rows, ignore_index=True)\n",
        "    final = final.sort_values('QueryDate').groupby('QueryDate', as_index=False).first()\n",
        "    final.to_csv(OUT_CSV, index=False, encoding=\"utf‑8\")\n",
        "\n",
        "    print(\"✅ Done!\")\n",
        "    print(\"Total trading days with at least 1 article:\", len(final))\n",
        "    print(\"Saved to:\", OUT_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting NINTENDO daily news download (no language filter)…\")\n",
        "    run_download()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1bQWOvhLRGg",
        "outputId": "0706d33f-bc28-4941-cfc4-8d261c73dc89"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting NINTENDO daily news download (no language filter)…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing days:  34%|███▍      | 746/2192 [18:35<1:16:52,  3.19s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20210118000000&enddatetime=20210118235959\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20210118000000&enddatetime=20210118235959\n",
            "Processing days:  35%|███▌      | 771/2192 [22:23<1:59:27,  5.04s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20210210000000&enddatetime=20210210235959\n",
            "Processing days:  35%|███▌      | 777/2192 [23:40<2:53:07,  7.34s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20210216000000&enddatetime=20210216235959\n",
            "Processing days:  71%|███████   | 1551/2192 [43:12<50:36,  4.74s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20230403000000&enddatetime=20230403235959\n",
            "Processing days:  83%|████████▎ | 1810/2192 [48:29<09:50,  1.55s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='api.gdeltproject.org', port=80): Read timed out. (read timeout=30)\")': /api/v2/doc/doc?query=%22Nintendo%22&mode=artlist&maxrecords=10&format=csv&startdatetime=20231218000000&enddatetime=20231218235959\n",
            "Processing days: 100%|██████████| 2192/2192 [1:02:25<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done!\n",
            "Total trading days with at least 1 article: 1561\n",
            "Saved to: nintendo_news_all_lang_2019_2024_1news_per_tradingday.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"nintendo_news_all_lang_2019_2024_1news_per_tradingday.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Xf8kbk6iLRpD",
        "outputId": "44375e4c-7408-442f-95c2-f8a0d0799d76"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e702af10-b304-4ef3-8309-14a8c3a8ff53\", \"nintendo_news_all_lang_2019_2024_1news_per_tradingday.csv\", 375776)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddGav9GChbHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKphYznjqauj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}